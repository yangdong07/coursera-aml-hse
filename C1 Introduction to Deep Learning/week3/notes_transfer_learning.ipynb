{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Image Retraining\n",
    "\n",
    "主要是做练习时候的一个困惑： \n",
    "\n",
    "使用已经训练好的 InceptionV3，对新的图片分类问题进行训练，**很容易** 产生过拟合：对训练集，很容易达到100%的准确率，但是对测试集/验证集，准确率不到 50%，并且很难继续提升。 这说明什么，说明\n",
    "\n",
    "google了一下，有些参考：\n",
    "\n",
    "- https://stackoverflow.com/questions/37605611/would-adding-dropout-help-reduce-overfitting-when-following-tensorflows-transfe\n",
    "\n",
    "- [Image Retraining](https://www.tensorflow.org/tutorials/image_retraining)\n",
    "\n",
    "这里按照 Image Retraining的教程，推演一遍，看看如何解决这个问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "main函数步骤：\n",
    "\n",
    "1. 准备一些模型参数\n",
    "2. 准备图片文件，包括训练集、验证集、测试集，以及标签\n",
    "3. （可选）对图片进行一些 distort， data argumentation\n",
    "4. 在原模型的某一层（bottleneck）后面添加 新的模型层（ retrain ），用于新的任务\n",
    "5. 在原模型的前面加上 `jepg_decoding`层，用于jpeg图片的解码\n",
    "6. 计算并缓存所有图片在原模型上bottleneck层的输出\n",
    "7. 在新的训练层之后添加 评估计算（`add_evaluation_step`）。\n",
    "8. 使用 bottleneck层输出的缓存，以及其他信息，训练新的模型层。\n",
    "9. 其他操作：存储模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        # Create the operations we need to evaluate the accuracy of our new layer.\n",
    "        evaluation_step, _ = add_evaluation_step(final_tensor, ground_truth_input)\n",
    "\n",
    "        # Merge all the summaries and write them out to the summaries_dir\n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train',\n",
    "                                             sess.graph)\n",
    "\n",
    "        validation_writer = tf.summary.FileWriter(\n",
    "            FLAGS.summaries_dir + '/validation')\n",
    "\n",
    "        # Create a train saver that is used to restore values into an eval graph\n",
    "        # when exporting models.\n",
    "        train_saver = tf.train.Saver()\n",
    "\n",
    "        # Set up all our weights to their initial default values.\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Run the training for as many cycles as requested on the command line.\n",
    "        for i in range(FLAGS.how_many_training_steps):\n",
    "            # Get a batch of input bottleneck values, either calculated fresh every\n",
    "            # time with distortions applied, or from the cache stored on disk.\n",
    "            if do_distort_images:\n",
    "                (train_bottlenecks,\n",
    "                 train_ground_truth) = get_random_distorted_bottlenecks(\n",
    "                    sess, image_lists, FLAGS.train_batch_size, 'training',\n",
    "                    FLAGS.image_dir, distorted_jpeg_data_tensor,\n",
    "                    distorted_image_tensor, resized_image_tensor, bottleneck_tensor)\n",
    "            else:\n",
    "                (train_bottlenecks,\n",
    "                 train_ground_truth, _) = get_random_cached_bottlenecks(\n",
    "                    sess, image_lists, FLAGS.train_batch_size, 'training',\n",
    "                    FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n",
    "                    decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\n",
    "                    FLAGS.architecture)\n",
    "            # Feed the bottlenecks and ground truth into the graph, and run a training\n",
    "            # step. Capture training summaries for TensorBoard with the `merged` op.\n",
    "            train_summary, _ = sess.run(\n",
    "                [merged, train_step],\n",
    "                feed_dict={bottleneck_input: train_bottlenecks,\n",
    "                           ground_truth_input: train_ground_truth})\n",
    "            train_writer.add_summary(train_summary, i)\n",
    "\n",
    "            # Every so often, print out how well the graph is training.\n",
    "            is_last_step = (i + 1 == FLAGS.how_many_training_steps)\n",
    "            if (i % FLAGS.eval_step_interval) == 0 or is_last_step:\n",
    "                train_accuracy, cross_entropy_value = sess.run(\n",
    "                    [evaluation_step, cross_entropy],\n",
    "                    feed_dict={bottleneck_input: train_bottlenecks,\n",
    "                               ground_truth_input: train_ground_truth})\n",
    "                tf.logging.info('%s: Step %d: Train accuracy = %.1f%%' %\n",
    "                                (datetime.now(), i, train_accuracy * 100))\n",
    "                tf.logging.info('%s: Step %d: Cross entropy = %f' %\n",
    "                                (datetime.now(), i, cross_entropy_value))\n",
    "                # TODO(suharshs): Make this use an eval graph, to avoid quantization\n",
    "                # moving averages being updated by the validation set, though in\n",
    "                # practice this makes a negligable difference.\n",
    "                validation_bottlenecks, validation_ground_truth, _ = (\n",
    "                    get_random_cached_bottlenecks(\n",
    "                        sess, image_lists, FLAGS.validation_batch_size, 'validation',\n",
    "                        FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n",
    "                        decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\n",
    "                        FLAGS.architecture))\n",
    "                # Run a validation step and capture training summaries for TensorBoard\n",
    "                # with the `merged` op.\n",
    "                validation_summary, validation_accuracy = sess.run(\n",
    "                    [merged, evaluation_step],\n",
    "                    feed_dict={bottleneck_input: validation_bottlenecks,\n",
    "                               ground_truth_input: validation_ground_truth})\n",
    "                validation_writer.add_summary(validation_summary, i)\n",
    "                tf.logging.info('%s: Step %d: Validation accuracy = %.1f%% (N=%d)' %\n",
    "                                (datetime.now(), i, validation_accuracy * 100,\n",
    "                                 len(validation_bottlenecks)))\n",
    "\n",
    "            # Store intermediate results\n",
    "            intermediate_frequency = FLAGS.intermediate_store_frequency\n",
    "\n",
    "            if (intermediate_frequency > 0 and (i % intermediate_frequency == 0)\n",
    "                and i > 0):\n",
    "                # If we want to do an intermediate save, save a checkpoint of the train\n",
    "                # graph, to restore into the eval graph.\n",
    "                train_saver.save(sess, CHECKPOINT_NAME)\n",
    "                intermediate_file_name = (FLAGS.intermediate_output_graphs_dir +\n",
    "                                          'intermediate_' + str(i) + '.pb')\n",
    "                tf.logging.info('Save intermediate result to : ' +\n",
    "                                intermediate_file_name)\n",
    "                save_graph_to_file(graph, intermediate_file_name, model_info,\n",
    "                                   class_count)\n",
    "\n",
    "        # After training is complete, force one last save of the train checkpoint.\n",
    "        train_saver.save(sess, CHECKPOINT_NAME)\n",
    "\n",
    "        # We've completed all our training, so run a final test evaluation on\n",
    "        # some new images we haven't used before.\n",
    "        run_final_eval(sess, model_info, class_count, image_lists, jpeg_data_tensor,\n",
    "                       decoded_image_tensor, resized_image_tensor,\n",
    "                       bottleneck_tensor)\n",
    "\n",
    "        # Write out the trained graph and labels with the weights stored as\n",
    "        # constants.\n",
    "        save_graph_to_file(graph, FLAGS.output_graph, model_info, class_count)\n",
    "        with gfile.FastGFile(FLAGS.output_labels, 'w') as f:\n",
    "            f.write('\\n'.join(image_lists.keys()) + '\\n')\n",
    "\n",
    "        export_model(model_info, class_count, FLAGS.saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant\n",
    "\n",
    "class FLAGS:\n",
    "    model_dir = '/home/yangdong/Downloads/tmp/imagenet/'\n",
    "    bottleneck_dir = '/tmp/bottleneck/'\n",
    "    summaries_dir  = '/tmp/retrain_logs'\n",
    "    final_tensor_name = 'final_result'\n",
    "    learning_rate = 0.01\n",
    "    how_many_training_steps = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangdong/Applications/miniconda3/envs/DL3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Looking for images in 'roses'\n",
      "INFO:tensorflow:Looking for images in 'sunflowers'\n",
      "INFO:tensorflow:Looking for images in 'daisy'\n",
      "INFO:tensorflow:Looking for images in 'dandelion'\n",
      "INFO:tensorflow:Looking for images in 'tulips'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.util import compat\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "MAX_NUM_IMAGES_PER_CLASS = 2 ** 27 - 1  # ~134M\n",
    "\n",
    "image_dir = '/home/yangdong/Study/Github/coursera-aml-hse/C1 Introduction to Deep Learning/week3/flower_photos'\n",
    "\n",
    "def create_image_lists(image_dir, testing_percentage=10, validation_percentage=10):\n",
    "    if not gfile.Exists(image_dir):\n",
    "        tf.logging.error(\"Image directory '\" + image_dir + \"' not found.\")\n",
    "        return None\n",
    "    result = {}\n",
    "    sub_dirs = [x[0] for x in gfile.Walk(image_dir)]\n",
    "    # The root directory comes first, so skip it.\n",
    "    is_root_dir = True\n",
    "    for sub_dir in sub_dirs:\n",
    "        if is_root_dir:\n",
    "            is_root_dir = False\n",
    "            continue\n",
    "        extensions = ['jpg', 'jpeg', 'JPG', 'JPEG']\n",
    "        file_list = []\n",
    "        dir_name = os.path.basename(sub_dir)\n",
    "        if dir_name == image_dir:\n",
    "            continue\n",
    "        tf.logging.info(\"Looking for images in '\" + dir_name + \"'\")\n",
    "        for extension in extensions:\n",
    "            file_glob = os.path.join(image_dir, dir_name, '*.' + extension)\n",
    "            file_list.extend(gfile.Glob(file_glob))\n",
    "        if not file_list:\n",
    "            tf.logging.warning('No files found')\n",
    "            continue\n",
    "        if len(file_list) < 20:\n",
    "            tf.logging.warning(\n",
    "                'WARNING: Folder has less than 20 images, which may cause issues.')\n",
    "        elif len(file_list) > MAX_NUM_IMAGES_PER_CLASS:\n",
    "            tf.logging.warning(\n",
    "                'WARNING: Folder {} has more than {} images. Some images will '\n",
    "                'never be selected.'.format(dir_name, MAX_NUM_IMAGES_PER_CLASS))\n",
    "        label_name = re.sub(r'[^a-z0-9]+', ' ', dir_name.lower())\n",
    "        training_images = []\n",
    "        testing_images = []\n",
    "        validation_images = []\n",
    "        for file_name in file_list:\n",
    "            base_name = os.path.basename(file_name)\n",
    "            # We want to ignore anything after '_nohash_' in the file name when\n",
    "            # deciding which set to put an image in, the data set creator has a way of\n",
    "            # grouping photos that are close variations of each other. For example\n",
    "            # this is used in the plant disease data set to group multiple pictures of\n",
    "            # the same leaf.\n",
    "            hash_name = re.sub(r'_nohash_.*$', '', file_name)\n",
    "            # This looks a bit magical, but we need to decide whether this file should\n",
    "            # go into the training, testing, or validation sets, and we want to keep\n",
    "            # existing files in the same set even if more files are subsequently\n",
    "            # added.\n",
    "            # To do that, we need a stable way of deciding based on just the file name\n",
    "            # itself, so we do a hash of that and then use that to generate a\n",
    "            # probability value that we use to assign it.\n",
    "            hash_name_hashed = hashlib.sha1(compat.as_bytes(hash_name)).hexdigest()\n",
    "            percentage_hash = ((int(hash_name_hashed, 16) %\n",
    "                                (MAX_NUM_IMAGES_PER_CLASS + 1)) *\n",
    "                               (100.0 / MAX_NUM_IMAGES_PER_CLASS))\n",
    "            if percentage_hash < validation_percentage:\n",
    "                validation_images.append(base_name)\n",
    "            elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "                testing_images.append(base_name)\n",
    "            else:\n",
    "                training_images.append(base_name)\n",
    "        result[label_name] = {\n",
    "            'dir': dir_name,\n",
    "            'training': training_images,\n",
    "            'testing': testing_images,\n",
    "            'validation': validation_images,\n",
    "        }\n",
    "    return result\n",
    "\n",
    "image_lists = create_image_lists(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roses 514 68 59\n",
      "sunflowers 559 71 69\n",
      "daisy 500 75 58\n",
      "dandelion 729 79 90\n",
      "tulips 624 79 96\n"
     ]
    }
   ],
   "source": [
    "for image_class in image_lists:\n",
    "    images = image_lists[image_class]\n",
    "    print(images['dir'], len(images['training']), len(images['testing']), len(images['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# inceptionV3 model\n",
    "def create_model_info():\n",
    "    \n",
    "    architecture = 'inception_v3'\n",
    "    is_quantized = False\n",
    "    # pylint: disable=line-too-long\n",
    "    data_url = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "    # pylint: enable=line-too-long\n",
    "    bottleneck_tensor_name = 'pool_3/_reshape:0'\n",
    "    bottleneck_tensor_size = 2048\n",
    "    input_width = 299\n",
    "    input_height = 299\n",
    "    input_depth = 3\n",
    "    resized_input_tensor_name = 'Mul:0'\n",
    "    model_file_name = 'classify_image_graph_def.pb'\n",
    "    input_mean = 128\n",
    "    input_std = 128\n",
    "    return {\n",
    "        'data_url': data_url,\n",
    "        'bottleneck_tensor_name': bottleneck_tensor_name,\n",
    "        'bottleneck_tensor_size': bottleneck_tensor_size,\n",
    "        'input_width': input_width,\n",
    "        'input_height': input_height,\n",
    "        'input_depth': input_depth,\n",
    "        'resized_input_tensor_name': resized_input_tensor_name,\n",
    "        'model_file_name': model_file_name,\n",
    "        'input_mean': input_mean,\n",
    "        'input_std': input_std,\n",
    "        'quantize_layer': is_quantized,\n",
    "    }\n",
    "\n",
    "model_info = create_model_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not extracting or downloading files, model already present in disk\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import urllib\n",
    "import tarfile\n",
    "def maybe_download_and_extract(data_url):\n",
    "    dest_directory = FLAGS.model_dir\n",
    "    if not os.path.exists(dest_directory):\n",
    "        os.makedirs(dest_directory)\n",
    "    filename = data_url.split('/')[-1]\n",
    "    filepath = os.path.join(dest_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\r>> Downloading %s %.1f%%' %\n",
    "                             (filename,\n",
    "                              float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        filepath, _ = urllib.request.urlretrieve(data_url, filepath, _progress)\n",
    "        print()\n",
    "        statinfo = os.stat(filepath)\n",
    "        tf.logging.info('Successfully downloaded %s %d bytes.', filename,\n",
    "                        statinfo.st_size)\n",
    "        print('Extracting file from ', filepath)\n",
    "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
    "    else:\n",
    "        print('Not extracting or downloading files, model already present in disk')\n",
    "        \n",
    "maybe_download_and_extract(model_info['data_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path:  /home/yangdong/Downloads/tmp/imagenet/classify_image_graph_def.pb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    model_path = os.path.join(FLAGS.model_dir, model_info['model_file_name'])\n",
    "    print('Model path: ', model_path)\n",
    "    with gfile.FastGFile(model_path, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        bottleneck_tensor, resized_input_tensor = (tf.import_graph_def(\n",
    "            graph_def,\n",
    "            name='',\n",
    "            return_elements=[\n",
    "                model_info['bottleneck_tensor_name'],\n",
    "                model_info['resized_input_tensor_name'],\n",
    "            ]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mul:0' shape=(1, 299, 299, 3) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized_input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "\n",
    "def add_final_retrain_ops(class_count, final_tensor_name, bottleneck_tensor,\n",
    "                          bottleneck_tensor_size, quantize_layer, is_training):\n",
    "    \"\"\"Adds a new softmax and fully-connected layer for training and eval.\n",
    "\n",
    "    We need to retrain the top layer to identify our new classes, so this function\n",
    "    adds the right operations to the graph, along with some variables to hold the\n",
    "    weights, and then sets up all the gradients for the backward pass.\n",
    "\n",
    "    The set up for the softmax and fully-connected layers is based on:\n",
    "    https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html\n",
    "\n",
    "    Args:\n",
    "      class_count: Integer of how many categories of things we're trying to\n",
    "          recognize.\n",
    "      final_tensor_name: Name string for the new final node that produces results.\n",
    "      bottleneck_tensor: The output of the main CNN graph.\n",
    "      bottleneck_tensor_size: How many entries in the bottleneck vector.\n",
    "      quantize_layer: Boolean, specifying whether the newly added layer should be\n",
    "          instrumented for quantized.\n",
    "      is_training: Boolean, specifying whether the newly add layer is for training\n",
    "          or eval.\n",
    "\n",
    "    Returns:\n",
    "      The tensors for the training and cross entropy results, and tensors for the\n",
    "      bottleneck input and ground truth input.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('input'):\n",
    "        bottleneck_input = tf.placeholder_with_default(\n",
    "            bottleneck_tensor,\n",
    "            shape=[None, bottleneck_tensor_size],\n",
    "            name='BottleneckInputPlaceholder')\n",
    "\n",
    "        ground_truth_input = tf.placeholder(\n",
    "            tf.int64, [None], name='GroundTruthInput')\n",
    "\n",
    "    # Organizing the following ops so they are easier to see in TensorBoard.\n",
    "    layer_name = 'final_retrain_ops'\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            initial_value = tf.truncated_normal(\n",
    "                [bottleneck_tensor_size, class_count], stddev=0.001)\n",
    "            layer_weights = tf.Variable(initial_value, name='final_weights')\n",
    "            variable_summaries(layer_weights)\n",
    "\n",
    "        with tf.name_scope('biases'):\n",
    "            layer_biases = tf.Variable(tf.zeros([class_count]), name='final_biases')\n",
    "            variable_summaries(layer_biases)\n",
    "\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            logits = tf.matmul(bottleneck_input, layer_weights) + layer_biases\n",
    "            tf.summary.histogram('pre_activations', logits)\n",
    "\n",
    "    final_tensor = tf.nn.softmax(logits, name=final_tensor_name)\n",
    "\n",
    "    # The tf.contrib.quantize functions rewrite the graph in place for\n",
    "    # quantization. The imported model graph has already been rewritten, so upon\n",
    "    # calling these rewrites, only the newly added final layer will be\n",
    "    # transformed.\n",
    "    if quantize_layer:\n",
    "        if is_training:\n",
    "            tf.contrib.quantize.create_training_graph()\n",
    "        else:\n",
    "            tf.contrib.quantize.create_eval_graph()\n",
    "\n",
    "    tf.summary.histogram('activations', final_tensor)\n",
    "\n",
    "    # If this is an eval graph, we don't need to add loss ops or an optimizer.\n",
    "    if not is_training:\n",
    "        return None, None, bottleneck_input, ground_truth_input, final_tensor\n",
    "\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        cross_entropy_mean = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels=ground_truth_input, logits=logits)\n",
    "\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy_mean)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
    "        train_step = optimizer.minimize(cross_entropy_mean)\n",
    "\n",
    "    return (train_step, cross_entropy_mean, bottleneck_input, ground_truth_input,\n",
    "            final_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在已训练网络上添加一层\n",
    "\n",
    "class_count = len(image_lists.keys())\n",
    "with graph.as_default():\n",
    "    (train_step, cross_entropy, bottleneck_input,\n",
    "     ground_truth_input, final_tensor) = add_final_retrain_ops(\n",
    "        class_count, FLAGS.final_tensor_name, bottleneck_tensor,\n",
    "        model_info['bottleneck_tensor_size'], model_info['quantize_layer'],\n",
    "        True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 在输入层添加 jpeg decoding层\n",
    "\n",
    "def add_jpeg_decoding(input_width, input_height, input_depth, input_mean,\n",
    "                      input_std):\n",
    "    \"\"\"Adds operations that perform JPEG decoding and resizing to the graph..\n",
    "\n",
    "    Args:\n",
    "      input_width: Desired width of the image fed into the recognizer graph.\n",
    "      input_height: Desired width of the image fed into the recognizer graph.\n",
    "      input_depth: Desired channels of the image fed into the recognizer graph.\n",
    "      input_mean: Pixel value that should be zero in the image for the graph.\n",
    "      input_std: How much to divide the pixel values by before recognition.\n",
    "\n",
    "    Returns:\n",
    "      Tensors for the node to feed JPEG data into, and the output of the\n",
    "        preprocessing steps.\n",
    "    \"\"\"\n",
    "    jpeg_data = tf.placeholder(tf.string, name='DecodeJPGInput')\n",
    "    decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\n",
    "    decoded_image_as_float = tf.cast(decoded_image, dtype=tf.float32)\n",
    "    decoded_image_4d = tf.expand_dims(decoded_image_as_float, 0)\n",
    "    resize_shape = tf.stack([input_height, input_width])\n",
    "    resize_shape_as_int = tf.cast(resize_shape, dtype=tf.int32)\n",
    "    resized_image = tf.image.resize_bilinear(decoded_image_4d,\n",
    "                                             resize_shape_as_int)\n",
    "    offset_image = tf.subtract(resized_image, input_mean)\n",
    "    mul_image = tf.multiply(offset_image, 1.0 / input_std)\n",
    "    return jpeg_data, mul_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 bottleneck层 的输出，并缓存。 主要是为了避免重复计算，加快训练速度\n",
    "def ensure_dir_exists(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "        \n",
    "def get_or_create_bottleneck(sess, image_lists, label_name, index, image_dir,\n",
    "                             category, bottleneck_dir, jpeg_data_tensor,\n",
    "                             decoded_image_tensor, resized_input_tensor,\n",
    "                             bottleneck_tensor, architecture):\n",
    "    \"\"\"Retrieves or calculates bottleneck values for an image.\n",
    "\n",
    "    If a cached version of the bottleneck data exists on-disk, return that,\n",
    "    otherwise calculate the data and save it to disk for future use.\n",
    "\n",
    "    Args:\n",
    "      sess: The current active TensorFlow Session.\n",
    "      image_lists: Dictionary of training images for each label.\n",
    "      label_name: Label string we want to get an image for.\n",
    "      index: Integer offset of the image we want. This will be modulo-ed by the\n",
    "      available number of images for the label, so it can be arbitrarily large.\n",
    "      image_dir: Root folder string of the subfolders containing the training\n",
    "      images.\n",
    "      category: Name string of which set to pull images from - training, testing,\n",
    "      or validation.\n",
    "      bottleneck_dir: Folder string holding cached files of bottleneck values.\n",
    "      jpeg_data_tensor: The tensor to feed loaded jpeg data into.\n",
    "      decoded_image_tensor: The output of decoding and resizing the image.\n",
    "      resized_input_tensor: The input node of the recognition graph.\n",
    "      bottleneck_tensor: The output tensor for the bottleneck values.\n",
    "      architecture: The name of the model architecture.\n",
    "\n",
    "    Returns:\n",
    "      Numpy array of values produced by the bottleneck layer for the image.\n",
    "    \"\"\"\n",
    "    label_lists = image_lists[label_name]\n",
    "    sub_dir = label_lists['dir']\n",
    "    sub_dir_path = os.path.join(bottleneck_dir, sub_dir)\n",
    "    ensure_dir_exists(sub_dir_path)\n",
    "    bottleneck_path = get_bottleneck_path(image_lists, label_name, index,\n",
    "                                          bottleneck_dir, category, architecture)\n",
    "    if not os.path.exists(bottleneck_path):\n",
    "        create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\n",
    "                               image_dir, category, sess, jpeg_data_tensor,\n",
    "                               decoded_image_tensor, resized_input_tensor,\n",
    "                               bottleneck_tensor)\n",
    "    with open(bottleneck_path, 'r') as bottleneck_file:\n",
    "        bottleneck_string = bottleneck_file.read()\n",
    "    did_hit_error = False\n",
    "    try:\n",
    "        bottleneck_values = [float(x) for x in bottleneck_string.split(',')]\n",
    "    except ValueError:\n",
    "        tf.logging.warning('Invalid float found, recreating bottleneck')\n",
    "        did_hit_error = True\n",
    "    if did_hit_error:\n",
    "        create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\n",
    "                               image_dir, category, sess, jpeg_data_tensor,\n",
    "                               decoded_image_tensor, resized_input_tensor,\n",
    "                               bottleneck_tensor)\n",
    "        with open(bottleneck_path, 'r') as bottleneck_file:\n",
    "            bottleneck_string = bottleneck_file.read()\n",
    "        # Allow exceptions to propagate here, since they shouldn't happen after a\n",
    "        # fresh creation\n",
    "        bottleneck_values = [float(x) for x in bottleneck_string.split(',')]\n",
    "    return bottleneck_values\n",
    "        \n",
    "def cache_bottlenecks(sess, image_lists, image_dir, bottleneck_dir,\n",
    "                      jpeg_data_tensor, decoded_image_tensor,\n",
    "                      resized_input_tensor, bottleneck_tensor, architecture):\n",
    "    how_many_bottlenecks = 0\n",
    "    ensure_dir_exists(bottleneck_dir)\n",
    "    for label_name, label_lists in image_lists.items():\n",
    "        for category in ['training', 'testing', 'validation']:\n",
    "            category_list = label_lists[category]\n",
    "            for index, unused_base_name in enumerate(category_list):\n",
    "                get_or_create_bottleneck(\n",
    "                    sess, image_lists, label_name, index, image_dir, category,\n",
    "                    bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,\n",
    "                    resized_input_tensor, bottleneck_tensor, architecture)\n",
    "\n",
    "                how_many_bottlenecks += 1\n",
    "                if how_many_bottlenecks % 100 == 0:\n",
    "                    tf.logging.info(\n",
    "                        str(how_many_bottlenecks) + ' bottleneck files created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Set up the image decoding sub-graph.\n",
    "    jpeg_data_tensor, decoded_image_tensor = add_jpeg_decoding(\n",
    "        model_info['input_width'], model_info['input_height'],\n",
    "        model_info['input_depth'], model_info['input_mean'],\n",
    "        model_info['input_std'])\n",
    "\n",
    "    cache_bottlenecks(sess, image_lists, \n",
    "                      FLAGS.image_dir, FLAGS.bottleneck_dir, \n",
    "                      jpeg_data_tensor, decoded_image_tensor, \n",
    "                      resized_image_tensor, bottleneck_tensor, FLAGS.architecture)\n",
    "\n",
    "    # Create the operations we need to evaluate the accuracy of our new layer.\n",
    "    evaluation_step, _ = add_evaluation_step(final_tensor, ground_truth_input)\n",
    "\n",
    "    # Merge all the summaries and write them out to the summaries_dir\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train',\n",
    "                                         sess.graph)\n",
    "\n",
    "    validation_writer = tf.summary.FileWriter(\n",
    "        FLAGS.summaries_dir + '/validation')\n",
    "\n",
    "    # Create a train saver that is used to restore values into an eval graph\n",
    "    # when exporting models.\n",
    "    train_saver = tf.train.Saver()\n",
    "\n",
    "    # Set up all our weights to their initial default values.\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Run the training for as many cycles as requested on the command line.\n",
    "    for i in range(FLAGS.how_many_training_steps):\n",
    "        # Get a batch of input bottleneck values, either calculated fresh every\n",
    "        # time with distortions applied, or from the cache stored on disk.\n",
    "        if do_distort_images:\n",
    "            (train_bottlenecks,\n",
    "             train_ground_truth) = get_random_distorted_bottlenecks(\n",
    "                sess, image_lists, FLAGS.train_batch_size, 'training',\n",
    "                FLAGS.image_dir, distorted_jpeg_data_tensor,\n",
    "                distorted_image_tensor, resized_image_tensor, bottleneck_tensor)\n",
    "        else:\n",
    "            (train_bottlenecks,\n",
    "             train_ground_truth, _) = get_random_cached_bottlenecks(\n",
    "                sess, image_lists, FLAGS.train_batch_size, 'training',\n",
    "                FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n",
    "                decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\n",
    "                FLAGS.architecture)\n",
    "        # Feed the bottlenecks and ground truth into the graph, and run a training\n",
    "        # step. Capture training summaries for TensorBoard with the `merged` op.\n",
    "        train_summary, _ = sess.run(\n",
    "            [merged, train_step],\n",
    "            feed_dict={bottleneck_input: train_bottlenecks,\n",
    "                       ground_truth_input: train_ground_truth})\n",
    "        train_writer.add_summary(train_summary, i)\n",
    "\n",
    "        # Every so often, print out how well the graph is training.\n",
    "        is_last_step = (i + 1 == FLAGS.how_many_training_steps)\n",
    "        if (i % FLAGS.eval_step_interval) == 0 or is_last_step:\n",
    "            train_accuracy, cross_entropy_value = sess.run(\n",
    "                [evaluation_step, cross_entropy],\n",
    "                feed_dict={bottleneck_input: train_bottlenecks,\n",
    "                           ground_truth_input: train_ground_truth})\n",
    "            tf.logging.info('%s: Step %d: Train accuracy = %.1f%%' %\n",
    "                            (datetime.now(), i, train_accuracy * 100))\n",
    "            tf.logging.info('%s: Step %d: Cross entropy = %f' %\n",
    "                            (datetime.now(), i, cross_entropy_value))\n",
    "            # TODO(suharshs): Make this use an eval graph, to avoid quantization\n",
    "            # moving averages being updated by the validation set, though in\n",
    "            # practice this makes a negligable difference.\n",
    "            validation_bottlenecks, validation_ground_truth, _ = (\n",
    "                get_random_cached_bottlenecks(\n",
    "                    sess, image_lists, FLAGS.validation_batch_size, 'validation',\n",
    "                    FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n",
    "                    decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\n",
    "                    FLAGS.architecture))\n",
    "            # Run a validation step and capture training summaries for TensorBoard\n",
    "            # with the `merged` op.\n",
    "            validation_summary, validation_accuracy = sess.run(\n",
    "                [merged, evaluation_step],\n",
    "                feed_dict={bottleneck_input: validation_bottlenecks,\n",
    "                           ground_truth_input: validation_ground_truth})\n",
    "            validation_writer.add_summary(validation_summary, i)\n",
    "            tf.logging.info('%s: Step %d: Validation accuracy = %.1f%% (N=%d)' %\n",
    "                            (datetime.now(), i, validation_accuracy * 100,\n",
    "                             len(validation_bottlenecks)))\n",
    "\n",
    "        # Store intermediate results\n",
    "        intermediate_frequency = FLAGS.intermediate_store_frequency\n",
    "\n",
    "        if (intermediate_frequency > 0 and (i % intermediate_frequency == 0)\n",
    "            and i > 0):\n",
    "            # If we want to do an intermediate save, save a checkpoint of the train\n",
    "            # graph, to restore into the eval graph.\n",
    "            train_saver.save(sess, CHECKPOINT_NAME)\n",
    "            intermediate_file_name = (FLAGS.intermediate_output_graphs_dir +\n",
    "                                      'intermediate_' + str(i) + '.pb')\n",
    "            tf.logging.info('Save intermediate result to : ' +\n",
    "                            intermediate_file_name)\n",
    "            save_graph_to_file(graph, intermediate_file_name, model_info,\n",
    "                               class_count)\n",
    "\n",
    "    # After training is complete, force one last save of the train checkpoint.\n",
    "    train_saver.save(sess, CHECKPOINT_NAME)\n",
    "\n",
    "    # We've completed all our training, so run a final test evaluation on\n",
    "    # some new images we haven't used before.\n",
    "    run_final_eval(sess, model_info, class_count, image_lists, jpeg_data_tensor,\n",
    "                   decoded_image_tensor, resized_image_tensor,\n",
    "                   bottleneck_tensor)\n",
    "\n",
    "    # Write out the trained graph and labels with the weights stored as\n",
    "    # constants.\n",
    "    save_graph_to_file(graph, FLAGS.output_graph, model_info, class_count)\n",
    "    with gfile.FastGFile(FLAGS.output_labels, 'w') as f:\n",
    "        f.write('\\n'.join(image_lists.keys()) + '\\n')\n",
    "\n",
    "    export_model(model_info, class_count, FLAGS.saved_model_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
